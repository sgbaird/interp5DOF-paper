% start the document

% specify the document layout and font size
\documentclass[preprint,12pt]{elsarticle}
% \documentclass[final,twocolumn,12pt]{elsarticle}
% \input{pkgs}
\usepackage[margin=1.5cm,includefoot]{geometry}
\usepackage{auto-paper}
% \PassOptionsToPackage{refcheck}{auto-paper} %comment this out before submission

\zexternaldocument*{main-frankenstein-2} %try deleting log files if producing an error, see https://tex.stackexchange.com/questions/131709/unclean-aux-file-causes-file-ended-while-scanning-use-of-newlbel-error-wh
\input{values.tex}
\input{abbrev.tex}

% Add "S" to figure captions, sections, and equations
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thetable}{S\arabic{table}}

\begin{document}
	\sloppy %maybe deals with figure/text spacing. Should deal with text going off the page
	
	\begin{frontmatter}
		
		%\title{Grain Boundary Octonion Meshing and Interpolation}
		\title{\mytitletwo{}: Supplementary Information}
		
		\input{authors}
		
	\end{frontmatter}
	
\tableofcontents

\section{Brief Summary of \glsxtrshort{vfz} Methods}
    \label{sec:supp:vfz-summary}

    We summarize the following aspects of the \gls{vfz} framework:
    \begin{itemize}
        \item Creation and definition of a \gls{vfz}
        \item Mapping \glspl{gb} to the \gls{vfz}
        \item Distance calculations
        \item Interpolation
        \item Comparison with traditional \gls{gbo} metric
    \end{itemize}
    Each of these is described in greater detail in \citet{bairdFiveDegreeofFreedomPropertyUnderReview}.
    
    To define a \gls{vfz}, an arbitrary, fixed, low-symmetry reference \gls{gbo} is chosen ($o_{\text{ref}}$) and for our use of \glspl{gbo}, the \gls{vfz} is defined as the region of $\mathbb{S}^7$ (the unit 7-sphere in 8 dimensions) closer to $o_{\text{ref}}$ than any of its symmetric images. If a low-symmetry \gls{gb} is chosen, the point within a \gls{vfz} will be unique within numerical tolerance (and hence it is a true \gls{fz}). Additionally, we use a Euclidean approximation to the true geodesic distance.
    
    
    A \gls{gbo} is composed of two quaternions, with the \gls{bp} normal in the $+z$ direction \cite{francisGeodesicOctonionMetric2019}. A \gls{gbo} is mapped into a \gls{vfz} by calculating the pairwise distances between the reference \gls{gbo} and each of the \glspl{seo}\footnote{Contrary to \citet{francisGeodesicOctonionMetric2019} which uses the passive convention for misorientation, we employ the active convention \cite{bairdFiveDegreeofFreedomPropertyUnderReview}. } and taking the \gls{seo} closest to the reference \gls{gbo}.
    
    Once a \gls{gbo} has been mapped into a \gls{vfz}, distance calculations proceed without further consideration of \glspl{seo}. The \gls{vfz} framework suffers from occasional, large distance overestimation which imposes a local sparseness of data and lead to poorer interpolation near the borders of a \gls{vfz}. However, this can be mitigated through ensemble or data augmentation techniques.
    
    Note that in the original definition of the \gls{vfz}, the authors examined different interpolation techniques \cite{bairdFiveDegreeofFreedomPropertyUnderReview}. In the present work, we focus on \gls{gpr} which in our case imposes the assumption that crystallographically similar \glspl{gb} share similar \glspl{gbe} within some correlation length. \Gls{gpr} has the added benefit of built-in uncertainty quantification.
    
    The primary differences between the \gls{vfz} framework and traditional \gls{gbo} distance metric are that the \gls{vfz} framework is defined by a continuous set of points, exhibits occasional distance overestimation, uses a Euclidean approximation, and has a lower computational complexity.

% \section{Nearest Neighbor Distances within a Large Point Set}

%     \Cref{fig:nnhist-knn-50000}a and b show a histogram of \num{50000} first \gls{nn} distances and the average distance for the k-th \gls{nn}.
    
% 	\begin{figure*}
% 		\centering
% 		\includegraphics[scale=1]{nnhist-knn-50000.png}
% 		\caption{(a) Histogram of \gls{nn} \gls{gbo} distances ($\omega$) in a \gls{vfzgbo} set of \num{50000} points. The average \gls{nn} distance was \SI{\nnomega}{\degree}. (b) The average k-th nearest neighbor distances demonstrate that \glspl{nn} up to the $\sim$\num{10}-th \gls{nn} fall within a tolerance of $\sim$\SI{5}{\degree}. Standard deviation uncertainty bars using approximately 10 trial runs are also shown. }
% 		\label{fig:nnhist-knn-50000}
% 	\end{figure*}


\section{Semivariograms for Estimating Global and Local Correlation Lengths} \label{sec:supp:semivariogram}

    \subsection{Semivariogram Method}

    First, we provide some background to put correlation lengths in context. The construction of \gls{gpr} models involves the combination of a prior distribution over the model space, with some set of observations and their quantified uncertainty. The result is a posterior distribution that provides the probability (density) of any particular model in light of the observed data and priors. In \gls{gpr}, as the name suggests, the priors are assumed to be Gaussian and therefore of the form
    \begin{equation}
        f\!\left(\mathbf{m}\right) \propto \exp{\left(-\frac{1}{2} {\left(\mathbf{m}-\mathbf{m}_0\right)}^{\mathsf{T}} \mathbf{C}_{\mathbf{m}_0}^{-1} {\left(\mathbf{m}-\mathbf{m}_0\right)}\right)}
    \end{equation}
    where $f\!\left(\mathbf{m}\right)$ is the probability density of an arbitrary model, $\mathbf{m} = \mathbf{m}\!\left(x\right)$, and $\mathbf{m}_0 = \mathbf{m}_0\!\left(x\right)$ is the prior model (i.e. a guess as to what the true model ought to look like). The quantity $\mathbf{C}_{\mathbf{m}_0} = \mathbf{C}_{\mathbf{m}_0}\!\left(x_i,x_j\right)$ is the "kernel" or covariance function of the prior and it describes the prior (assumed) covariance between the values $\mathbf{m}\!\left(x_i\right)$ and $\mathbf{m}\!\left(x_j\right)$.
    
    It is possible to use a wide variety of kernel types, depending on the prior information one may have about the physical phenomenon one is attempting to model, e.g. continuity, differentiability, anisotropy, stationarity, and length-scales of correlation. One of the most common kernels employed is the Gaussian (sometimes called the squared exponential) kernel:
    \input{equations/squared-exponential}
    
    % estimate $\sigma$ and $l$ from the data.
    The values of $\sigma$ and $l$ are typically estimated from the data. One approach, employed by the \texttt{fitrgp()} routine in MATLAB involves numerical optimization via gradient descent which maximizes the likelihood as a function of these parameters \cite{ExactGPRMethod}.
    
    % or estimate using empirical semivariograms with a stationary Gaussian kernel.
    An alternative approach, adapted from geostatistical applications, involves calculation of the empirical semivariogram [REF], in which one bins the space of pairwise distances between all GBs and then calculates half the average pairwise variance of the corresponding property values in each bin:
    \begin{equation}
        \label{eq:semivariogram}
        \kappa\!\left(d_k\right) = \frac{1}{2N_k} \sum_{\substack{d_\Omega\left(x_i,x_j\right) \in \\ \left[d_k^-,d_k^+\right]}} {\left| E\!\left(x_i\right) - E\!\left(x_j\right)\right|}^2
    \end{equation}
    where $x_i$ and $x_j$ are the crystallographic coordinates of GBs $i$ and $j$, $d_\Omega\left(x_i,x_j\right)$ is the distance between them, and $E\!\left(x_i\right)$ and $E\!\left(x_i\right)$ are their respective energies. $d_k$ is the location (distance) of the $k$-th bin center having left- and right-hand limits $d_k^-$ and $d_k^+$, and $N_k$ is the number of measurement pairs whose distance falls in the $k$-th bin. Due to limited sampling of large distances and the fact that the most informative part of the semivariogram is the region near $d_k = 0$, it is customary to limit the semivariogram to half of the maximum distance [REF]. The empirical semivariogram is then fit with an analytical model to obtain the parameters of the kernel (covariance) function taking advantage of the relationship
    \begin{equation}
        \label{eq:analyticalsemivariogrammodel}
        \kappa\!\left(d\right) = \sigma_f^2 - \mathbf{C}_{\mathbf{m}_0}\!\left(d\right)
    \end{equation}
    where we have made explicit the stationarity of the Gaussian kernel (i.e. that it depends only the distance between two points, not on their respective locations).
    
    % choice of correlation strength
    Having obtained a value for the length scale kernel parameter $l$, one can define a correlation length for the data. If the distance between two points is equal to $l$ the kernel function indicates that their correlation will be equal to $\rho = \exp{\left(-1/2\right)} \approx 0.61$. However, one might reasonably want to know the length scale over which GB properties are correlated by a different amount. In general for a specified correlation strength, $\rho$, the corresponding correlation length is given by
    \begin{equation}
        \label{eq:generalizedcorrelationlength}
        l'\!\left(\rho\right) = l \sqrt{-2 \ln{\rho}}
    \end{equation}
    We will refer to the parameter $l$ as \emph{the} correlation length, but one can use \cref{eq:generalizedcorrelationlength} to determine the length scale corresponding to any specified correlation strength. %1-2 paragraphs

%     \subsection{Global Correlation Lengths} \label{sec:supp:semivariogram:global}
% 	One of the most fundamental observations comes from the shape of the empirical semivariograms. As mentioned in \cref{sec:methods:correlation}, there are a wide variety of kernels that could be candidates for modeling correlations in different systems. Different kernels are used to capture different types of correlations, and each has a characteristic signature that can be observed in the semivariogram. For example, when a system exhibits exponential-type correlations, the semivariogram manifests this in the form of an exponential convergence towards an asymptotic constant value at long-distances. Linear and power-type correlations manifest an absence of a long-distance plateau. In the present system, there is a clear change in concavity in the semivariogram, this is a signature of correlations that are Gaussian in nature. Thus, we find that GB energy correlations in these systems are Gaussian, and should therefore be modeled using Gaussian kernels.
% 	\begin{figure*}
% 	    \centering
% 	    \includegraphics[scale=0.75]{figures/GlobalCorrelationLengthVariograms.png}
% 	    \caption{Empirical semivariograms (markers) for the (a) Ni and (b) Fe datasets. Solid lines show the fits of the analytical semivariogram models (\cref{eq:analyticalsemivariogrammodel}).}
% 	    \label{fig:globalvariogramfits}
% 	\end{figure*}
	
	\subsection{Local Correlation Lengths} \label{sec:supp:semivariogram:local}

    \begin{figure*}
        \centering
        \includegraphics[scale=1]{figures/LocalCorrelationLengthVariogramsOlmsted.png}
        \caption{Local empirical semivariograms (markers), respectively centered at various low-$\Sigma$ GBs for the Ni dataset. Solid lines show the fits of the analytical semivariogram models. In (d) the one point marked with an $\times$ was considered an outlier and was excluded from the fit.}
        \label{fig:localvariogramsolmsted}
    \end{figure*}
    \begin{figure*}
        \centering
        \includegraphics[scale=1]{figures/LocalCorrelationLengthVariogramsKim.png}
        \caption{Local empirical semivariograms (markers), respectively centered at various low-$\Sigma$ GBs for the Fe dataset. Keys for which \glspl{gb} each of these correspond to in the original papers are given for Fe and Ni in \cref{tab:sigma-key-olmsted,tab:sigma-key-kim}, respectively. Solid lines show the fits of the analytical semivariogram models.}
        \label{fig:localvariogramskim}
    \end{figure*}
    The local empirical semivariograms are noisier than the global empirical semivariograms, likely due to considering fewer GB pairs. Nevertheless, reasonable fits were obtained for most of the GBs with the exception of the $\Sigma 5$ GB in the Ni dataset. In many of the local semivariograms we again see the signature change in concavity suggesting that the local correlations in the vicinity of these GBs are also Gaussian in nature. However, there are some exceptions where the nature of the correlations is more ambiguous. We anticipate that this ambiguity could be resolved with datasets that are either larger (compared to the Ni dataset) or having less noise (compared to the Fe dataset) than those considered here. However, the local empirical semivariograms seem to be generally consistent with Gaussian-type correlations.
    
    The traditional Gaussian kernel exhibits the property of stationarity, meaning that the covariance depends only on the distance between two points, not on their respective locations (i.e. $\mathbf{C}_{\mathbf{m}_0}\!\left(x_i,x_j\right) = \mathbf{C}_{\mathbf{m}_0}\!\left(d\left(x_i,x_j\right)\right)$). The use of a stationary kernel implies a prior assumption that there is a single global correlation length that applies everywhere. The fact that we observe significant variation in correlation length across the \gls{gb} character space suggests that it would be better to employ non-stationary kernels (this is why, when referring to the global correlation length results presented earlier, we were careful to say that the global correlation lengths hold "on average" across the space). In particular, due to the fact that the local semivariograms do seem to be generally consistent with Gaussian-type correlations, we suggest that the non-stationary version of the Gaussian kernel [REF] may be a reasonable choice. One additional potential benefit of employing non-stationary kernels might be improved resolution of cusps in the \gls{gb} energy landscape.


\section{Fe Input Dataset Characteristics}
    We describe the methods and results used to evaluate the quality of the noisy Fe dataset (\cref{sec:supp:kim-interp:quality}) and discuss intrinsic uncertainty (\cref{sec:results:lit:error}) and overprediction bias (\cref{sec:results:lit:overprediction}). Finally, we offer suggestions on how to improve on existing datasets (\cref{sec:results:lit:improve}).
    
    \subsection{Methods and Results}
	\label{sec:supp:kim-interp:quality}
	Of the $\sim$\num{60000} \glspl{gb}\footnote{The "no-boundary" \glspl{gb} (i.e. \glspl{gb} with close to \SI{0}{\joule\per\square\meter} \gls{gbe}) were removed before testing for degeneracy.} in \cite{kimPhasefieldModeling3D2014}, $\sim$\num{10000} \glspl{gb} were repeats that were identified by converting to \glspl{vfzgbo} and applying \vfzorepo{} function \texttt{avg\_repeats.m}. In \cite{kimPhasefieldModeling3D2014}, mechanically selected \glspl{gb} were those which involved sampling in equally spaced increments\footnote{In some cases, this was equally spaced increments of the argument of a trigonometric function.} for each \gls{5dof} parameter, and a few thousand intentionally selected \glspl{gb} (i.e. special \glspl{gb}) were also considered. Of mechanically and intentionally selected \glspl{gb}, \numlist{9170;112} are repeats, respectively, with a total of \num{2496} degenerate sets\footnote{A degenerate "set" is distinct from a \gls{vfzgbo} "set", the latter of which is often used in the main text.} (see \cref{fig:kim-interp-degeneracy-sets} for a degeneracy histogram). Thus, on average there is a degeneracy of approximately four per set of degenerate \glspl{gb}.
	
	By comparing \gls{gbe} values of (unintentionally\footnote{To our knowledge, the presence of repeat \glspl{gb} were not mentioned in \cite{kimPhasefieldModeling3D2014} or \cite{kimIdentificationSchemeGrain2011}}) repeated \glspl{gb} in the Fe simulation dataset \cite{kimPhasefieldModeling3D2014}, we can estimate the intrinsic error of the \inpt{} data. For example, minimum and maximum deviations from the average value of a degenerate set are \SIlist{-0.2625;0.2625}{\joule\per\square\meter}, respectively, indicating that a repeated Fe \gls{gb} simulation from \cite{kimPhasefieldModeling3D2014} can vary by as much as \SI{0.525}{\joule\per\square\meter}, though rare. Additionally, \Gls{rmse} and \gls{mae} values can be obtained within each degenerate set by comparing against the set mean. Overall \gls{rmse} and \gls{mae} are then obtained by averaging and weighting by the number of \glspl{gb} in each degenerate set. Following this procedure, we obtain an average set-wise \gls{rmse} and \gls{mae} of \SIlist{0.06529;0.06190}{\joule\per\square\meter}, respectively, which is an approximate measure of the intrinsic error of the data. \cref{fig:kim-interp-degeneracy-results} shows histograms and parity plots of the intrinsic error. The overestimation of intrinsic error mentioned in the main text (\cref{sec:results:lit:error}) could stem from bias as to what type of \glspl{gb} exhibit repeats based on the sampling scheme used in \cite{kimPhasefieldModeling3D2014} and/or that many of the degenerate sets contain a low number of repeats (\cref{fig:kim-interp-degeneracy-sets}).
	
	Next, we see that by binning \glspl{gb} into degenerate sets, most degenerate sets have a degeneracy of fewer than 5 \cref{fig:kim-interp-degeneracy-sets}. We split the repeated data into sets with a degeneracy of fewer than 5 and greater than or equal to 5 and plot the errors (relative to the respective set mean) in both histogram form (\cref{fig:kim-interp-degeneracy-results}a and \cref{fig:kim-interp-degeneracy-results}c, respectively) and as hexagonally-binned parity plots (\cref{fig:kim-interp-degeneracy-results}b and \cref{fig:kim-interp-degeneracy-results}d, respectively). While heavily repeated \glspl{gb} tend to give similar results, occasionally repeated \glspl{gb} often have larger \gls{gbe} variability. This could have physical meaning: Certain types of (e.g. high-symmetry) \glspl{gb} tend to have less variation (i.e. fewer and/or more tightly distributed metastable states). However, it could also be an artifact of the simulation setup that produced this data (e.g. deterministic simulation output for certain types of \glspl{gb}).
	
	\begin{figure}
		\centering
		\includegraphics[scale=1]{kim-interp-degeneracy-sets.png}
		\caption{Histogram of number of sets vs. number of degenerate \glspl{gb} per set for the Fe simulation dataset \cite{kimPhasefieldModeling3D2014}. Most sets have a degeneracy of fewer than 5.}
		\label{fig:kim-interp-degeneracy-sets}
	\end{figure}
	
	\begin{figure}
		\centering
		\includegraphics[scale=1]{kim-interp-degeneracy-results.png}
		\caption{Degenerate \glspl{gb} sets are split into those with a degeneracy of fewer than 5 and greater than or equal to 5 and plotted as ( (a) and (c), respectively) error histograms and ( (b) and (d), respectively) hexagonally-binned parity plots. Large degenerate sets tend to have very low error, whereas small degenerate sets tend to have higher error. In other words, \glspl{gb} that are more likely to be repeated many times based on the sampling scheme in \cite{kimPhasefieldModeling3D2014} tend to give similar results, whereas \glspl{gb} that are less likely to be repeated often have larger variability in the simulation output. We do not know if this has physical meaning or is an artifact of the simulation setup.}
		\label{fig:kim-interp-degeneracy-results}
	\end{figure}
		
	\subsection{Intrinsic Uncertainty} \label{sec:results:lit:error}
	We estimate the intrinsic uncertainty of an Fe \SI{0}{\kelvin} \gls{ms} simulation dataset to be \SIlist{0.06529;0.06190}{\joule\per\square\meter} depending on whether \gls{rmse} or \gls{mae} estimates are used, respectively. Minimum and maximum error was \SIlist{-0.2625;0.2625}{\joule\per\square\meter}, respectively.

	First, because only a single metastable state was used for each \gls{gbe} simulation, both the training and validation data are subject to noise, consistent with a wide lateral spread of predictions and the intrinsic uncertainty estimation (\cref{fig:kim-interp-degeneracy-results}). The Fe simulation dataset \gls{gprm} model gives lower \gls{rmse} (\SI{0.055035}{\joule\per\square\meter}) and \gls{mae} (\SI{0.039185}{\joule\per\square\meter}) than the uncertainty estimates. This indicates that the uncertainty itself is somewhat overestimated\footnote{The \outpt{} error of a model typically cannot be less than the noise of the \outpt{} data of a model even if the model is estimating the true \outpt{} values with better accuracy than the noise (which is very possible and even expected with \gls{gpr} models when the noise in the \inpt{} data is approximately Gaussian).}. The fact that both model and uncertainty metrics are relatively close and the prediction \cite{bairdFiveDegreeofFreedomPropertyUnderReview} and uncertainty parity plots (\cref{fig:kim-interp-degeneracy-results}b) are similar suggests that the model is performing well. It also suggests that further improvements in the model relative to the "true" values will be "hidden", i.e. they will probably not manifest as lower \gls{rmse} or \gls{mae} nor as more tightly distributed parity plots despite improving performance "behind the scenes".

    \subsection{Overprediction Bias} \label{sec:results:lit:overprediction}
	Next, given the theoretical existence of a true minimum \gls{gbe} for a given \gls{gb}, the predictions which were based on metastable \glspl{gbe} can be assumed to have an overprediction bias relative to the true minimum. On average, we expect this overprediction bias relative to the true minimum \gls{gbe} (rather than the most likely metastable state) may be on the order of a few hundred \SI{}{\milli\J\per\square\m} and may vary as a function of true minimum \gls{gbe}. In other words, the model obtained is probably an estimate of the most likely metastable \gls{gbe} rather than the true minimum \gls{gbe}. This is akin to saying that we obtain from this data a model that approximates the non-equilibrium, Stillinger quenched red curve of Figure 4(c1) in \cite{hanGrainboundaryMetastabilityIts2016}, not the minimum \gls{gbe} blue curve of the same chart. See \cite{hanGrainboundaryMetastabilityIts2016} for an in-depth treatment of equilibrium and metastable \gls{gbe}.

	\subsection{Improving on Existing Datasets} \label{sec:results:lit:improve}
	Finally, datasets where multiple metastable \glspl{gbe} (e.g. 3-10 repeats) are provided for each \gls{gb} will likely greatly improve the performance of the \gls{gpr} model in predicting either the most likely metastable \gls{gbe} (when all \glspl{gbe} are considered) or the true minimum \gls{gbe} (when only the minimum \gls{gbe} is considered for each \gls{gb}) and may even negate the need for a \gls{gprm} approach. Thus, it is suggested that, where feasible, future large-scale \gls{gb} bicrystal simulation studies report all property data for repeated trial runs rather than a single trial run or a single value from a set of trial runs. Ideally, data for the three additional microscopic \glspl{dof} for \glspl{gb} (which falls into the category of epistemic uncertainty in this work) would also be included. We believe it is likely that minimum energy paths (i.e. paths of steepest descent) in the \gls{gbe} landscape depend on both macroscopic and microscopic \glspl{dof} (in total, 8DOF) and could offer a more holistic view of \gls{gb} behavior that better mimics and explains experimental grain growth observations. Indeed, it has been experimentally observed that at least some \gls{gb} migration mechanisms involve structural transformations between equilibrium \glspl{gb} via metastable states \cite{weiDirectImagingAtomistic2021}.
	

\section{Gridded Sampling for Numerical Differentation} \label{sec:supp:grid}
An isotropically sized \gls{fz} may be easier to uniformly discretize than a high aspect-ratio space (i.e. a fixed discretization length can be used across all dimensions). What this doesn't describe, however, is curvature. In order to create a gridded array, which is important for numerical differentiation, a hypercube with each primary axis oriented with Euclidean dimensions is to be preferred. As curvature or misalignment is introduced as may be expected with a \gls{vfz} point cloud, \glspl{gb} outside of the \gls{vfz} will necessarily be sampled; this phenomena will be exaggerated in high dimensions\footnote{For perspective, a discretization into 9 segments (10 points) along each dimension will have a spacing of $\sim$\SI{7}{\degree} and require \num{1e5} grid points. In order to achieve a more reasonable grid spacing of $\sim$\SI{2}{\degree}, a minimum of $\sim$\num{24} discretizations (25 points) along each dimension is necessary and will produce $\sim$\num{1e7} grid points. }. Fortunately, most of the information is contained in the first five dimensions after \gls{svd} transformation (\cref{sec:results:dimensions}). Thus, the latter three dimensions can likely be ignored without substantially affecting e.g. an interpolation or numerical differentiation scheme.

\section{\glsxtrshort{gb}s Used for Path Visualization} \label{sec:supp:sigma-key}

See \cref{tab:sigma-key-olmsted} and \cref{tab:sigma-key-kim} for the \citet{olmstedSurveyComputedGrain2009} and \citet{kimPhasefieldModeling3D2014} datasets, respectively.

\begin{table}[!htb]
    \centering
    \caption{Minimum $\Sigma$ (Sigma) \glspl{gb} and corresponding IDs used for path visualization within the original \citet{olmstedSurveyComputedGrain2009} dataset. }
    \label{tab:sigma-key-olmsted}
    \csvautobooktabular{tables/sigma-key-olmsted.csv}
\end{table}

\begin{table}[!htb]
    \centering
    \caption{Minimum $\Sigma$ (Sigma) \glspl{gb} and corresponding IDs used for path visualization within the original \citet{kimPhasefieldModeling3D2014} dataset. }
    \label{tab:sigma-key-kim}
    \csvautobooktabular{tables/sigma-key-kim.csv}
\end{table}

\printglossary

\clearpage
\bibliographystyle{elsarticle-num-names}
\bibliography{5dof-gb-energy.bib}

\end{document}